# Time and Space Complexity - Sample study note generated by AI

## What is Time Complexity?

Time complexity measures how the runtime of an algorithm grows as the input size increases. We use Big O notation to describe the worst-case scenario. It helps us compare algorithms and predict performance with large datasets.

## What is Space Complexity?

Space complexity measures how much memory an algorithm uses relative to input size. This includes both the space for input data and any extra space the algorithm needs to work.

## Big O Notation Basics

We focus on the dominant term and ignore constants. If an algorithm takes 5n² + 3n + 7 steps, we say it's O(n²) because n² grows much faster than n or constant terms when n gets large.

## Common Time Complexities

**O(1) - Constant Time** Takes same time regardless of input size. Examples: accessing array element by index, basic math operations, hash table lookup in best case.

**O(log n) - Logarithmic Time** Very efficient, grows slowly. Examples: binary search, finding element in balanced binary search tree. If you double input size, you only add one more step.

**O(n) - Linear Time** Time grows directly with input size. Examples: linear search through array, printing all elements, finding maximum value in unsorted array.

**O(n log n) - Linearithmic Time** Common in good sorting algorithms. Examples: merge sort, heap sort, quick sort average case. Much better than O(n²) but slower than O(n).

**O(n²) - Quadratic Time** Time grows with square of input size. Examples: bubble sort, selection sort, nested loops checking all pairs. Gets slow quickly with large inputs.

**O(2ⁿ) - Exponential Time** Extremely slow for large inputs. Examples: recursive fibonacci without memoization, trying all subsets of a set. Avoid if possible.

## Space Complexity Examples

**O(1) Space** - Algorithm uses same amount of extra memory regardless of input size. Examples: swapping two variables, iterative algorithms that only use a few variables.

**O(n) Space** - Memory usage grows with input size. Examples: creating copy of array, recursive algorithms (call stack), merge sort temporary arrays.

**O(log n) Space** - Usually from recursion depth. Examples: binary search recursive implementation, balanced tree operations.

## Analyzing Algorithms

For loops: if loop runs n times, that's O(n). Nested loops multiply complexities together.

Recursion: look at how many times function calls itself and how much work each call does. Tree-like recursion can be exponential.

## Trade-offs

Sometimes we can trade time for space or vice versa. Dynamic programming uses more memory to avoid recalculating same values. Hash tables use extra space to get faster lookups.

## Practical Tips

- O(1) and O(log n) are excellent for any input size
- O(n) and O(n log n) are good for most practical purposes
- O(n²) starts getting slow around 10,000 elements
- O(2ⁿ) is only practical for very small inputs (maybe 20-30 elements)

## Appendix
**Common Mistakes:**

Confusing best case with worst case. Hash tables are O(1) average but O(n) worst case. Always consider worst case for Big O analysis.

**Important Questions:**

How does choice of data structure affect complexity? When might we prefer a slower algorithm? What's the relationship between recursion depth and space complexity?

**Examples to Remember:**

Binary search: O(log n) time, O(1) space iterative or O(log n) space recursive Merge sort: O(n log n) time, O(n) space Bubble sort: O(n²) time, O(1) space Fibonacci recursive: O(2ⁿ) time, O(n) space